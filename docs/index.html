<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>nano4M â€“ Audio Modality Extension</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>

  <!-- Contenu principal centrÃ© -->
  <div class="wrapper">
    <div class="title-section">
      <h1>Integrating Audio into nano4M: 
        A Multimodal Extension</h1>
      <div class="authors">
        <a href="https://github.com/petritarifi" target="_blank">Petrit Arifi</a>,
        <a href="https://github.com/ozairfaizan" target="_blank">Ozair Faizan</a>,
        <a href="https://github.com/ylanv" target="_blank">Ylan Vifian</a>
      </div>
      <div class="affiliation">
        Swiss Federal Institute of Technology Lausanne (EPFL)

      </div>
      <div class="button-row">
        <a href="https://github.com/ylanv/nano4M" target="_blank" class="button">ðŸ”— Code</a>
      </div>
    </div>
  </div>

  <!-- Bandeau abstract -->
  <section class="abstract-section">
    <h2>Abstract</h2>
    <p>
      We extend <strong>nano4M</strong> by injecting an <em>audio</em> streamâ€”tokenised with our custom VQ-VAEâ€”to train a truly
      <em>multimodal</em> foundation model. This lightweight prototype shows that even small Transformer backbones can
      learn meaningful cross-modal representations, enabling zero-shot <abbr title="Text-to-Speech">TTS</abbr> and audio-conditioned image
      generation. We describe the audio tokenizer, the aligned dataset we built from AudioCaps, and the training
      tweaks that made the model converge despite limited compute.
    </p>
  </section>

  <!-- Bandeau overview -->
  <section class="overview-section">
    <h2>Overview</h2>
    <div class="overview-cards">
      <a class="card" href="#intro">Introduction</a>
      <a class="card" href="#audio-tokenizer">Audio tokenizer</a>
      <a class="card" href="#dataset">Dataset</a>
      <a class="card" href="#nanofm">Nano4M & Extras</a>
      <a class="card" href="#conclusion">Conclusion & Limitations</a>
    </div>
  </section>

  <!-- Contenu de sections (Ã  remplir si tu veux que les boutons scrollent vers quelque chose) -->
  <div class="wrapper">
    <div class="section" id="intro">
      <h2>Introduction</h2>
      <p>...</p>
    </div>

    <div class="section" id="audio-tokenizer">
      <h2>Audio Tokenizer</h2>
      <p>...</p>
    </div>

    <div class="section" id="dataset">
      <h2>Dataset</h2>
      <p>...</p>
    </div>
    <div class="section" id="nanofm">
      <h2>Nano4M & Extras</h2>
      <p>
        We used the <strong>AudioCaps</strong> dataset mentioned above to train our multimodal model, <strong>nano4M</strong>. To do this, we applied three different tokenization strategies:
      </p>
      <ul>
        <li><strong>GPT-2</strong> for the text captions,</li>
        <li><strong>Cosmos</strong> for RGB images (patch-based tokenization),</li>
        <li>and our own <strong>VQ-VAE-based tokenizer</strong> for the audio.</li>
      </ul>
      <p>
        Once the dataset was fully tokenized, we trained our model using the <strong>nano4M architecture</strong>, which is based on a <strong>masked Transformer</strong>. This means that during training, we intentionally hide some parts of the input â€” and ask the model to guess whatâ€™s missing. MANQUE IMAGE ARCHITECTURE
      </p>
      <p>
        These missing parts can come from <strong>any of the three modalities</strong>: text (words), audio (sound tokens), or image (patches). The model has to make predictions using the remaining available information from the other parts â€” and possibly from the other modalities.
      </p>
      <p>
        For example, if we hide a piece of audio, the model can try to guess it using the corresponding image and caption. If we hide a word from the text, the model can use the sound and visual context to find what fits best.
      </p>
      <p>
        By repeating this process many times, the model learns to <strong>connect and understand</strong> how text, images, and audio relate to each other. This helps it build a <strong>shared representation</strong> of all three, and allows it to do things like <em>generate sound from text</em>, or <em>use audio to help interpret an image</em>.
      </p>
      <p>
        As a result, we obtained a <strong>lightweight but versatile model</strong> that can perform basic reasoning across modalities. Our early experiments show promising results, especially in tasks like <em>text-to-audio generation</em> and <em>cross-modal completion</em>.
      </p>
      <p>
        <strong>Below are some of the results we obtained during our evaluation.</strong>
        MANQUE RESULTATS
      </p>
    </div>
    <div class="section" id="conclusion">
      <h2>Conclusion & Limitations</h2>
      <p>...</p>
    </div>

    <div class
