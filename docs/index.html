<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>nano4M â€“ Audio Modality Extension</title>
  <link rel="stylesheet" href="style.css">
  <script type="text/javascript"
    id="MathJax-script"
    async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

</head>
<body>

  <!-- Contenu principal centrÃ© -->
  <div class="wrapper">
    <div class="title-section">
      <h1>Integrating Audio into nano4M: 
        A Multimodal Extension</h1>
      <div class="authors">
        <a href="https://github.com/petritarifi" target="_blank">Petrit Arifi</a>,
        <a href="https://github.com/ozairfaizan" target="_blank">Ozair Faizan</a>,
        <a href="https://github.com/ylanv" target="_blank">Ylan Vifian</a>
      </div>
      <div class="affiliation">
        Swiss Federal Institute of Technology Lausanne (EPFL)

      </div>
      <div class="button-row">
        <a href="https://github.com/ylanv/nano4M" target="_blank" class="button">ðŸ”— Code</a>
      </div>
    </div>
  </div>

  <!-- Bandeau abstract -->
  <section class="abstract-section">
    <h2>Abstract</h2>
    <p>
      <strong>Most models today</strong> can understand <strong>text and images</strong> but not <strong>sound</strong>. In this project, we taught our model <strong>nano4M</strong> to also work with audio. We built a tool that turns sounds into <strong>tokens</strong> so the model can learn from them just like it does with words and pictures. Since there was no dataset with all three types of data, including <strong>audio, images, and text</strong>, we created our own by combining real audio with captions and <strong>AI-generated images</strong>. In the end, our model learned to <strong>connect these different kinds of data</strong> and guess missing parts using the others. This shows that combining sound, text, and images in one model is <strong>possible</strong> and opens the door to <strong>new and useful applications</strong>.
    </p>
  </section>

  <!-- Bandeau overview -->
  <section class="overview-section">
    <h2>Overview</h2>
    <div class="overview-cards">
      <a class="card" href="#intro">Introduction</a>
      <a class="card" href="#audio-tokenizer">Audio tokenizer</a>
      <a class="card" href="#dataset">Dataset</a>
      <a class="card" href="#nanofm">Nano4M & Extras</a>
      <a class="card" href="#conclusion">Conclusion & Limitations</a>
    </div>
  </section>

  <!-- Contenu de sections (Ã  remplir si tu veux que les boutons scrollent vers quelque chose) -->
  <div class="wrapper">
    <div class="section" id="intro">
      <h2>Introduction</h2>
      <p>
      In this project, we worked on adding audio to our multimodal model, <strong>nano4M</strong>. Our goal was to turn audio signals into a sequence of tokens, just like how we do with text and images.
      To do this, we built an audio tokenizer using a model called <strong>VQ-VAE</strong>. This model helps us take raw audio and convert it into a shorter, discrete representation that a neural network can understand.
      </p>
      <p>
      We tested different versions of the tokenizer: one using spectrograms, one using raw waveforms, and one with a special decoder called <strong>WaveNet</strong> to improve the sound quality.
      </p>
      <p>
      To train nano4M on audio together with images and text, we needed a dataset that aligned all three modalities. Since no public dataset included audio, images, and captions simultaneously, we <strong>created our own multimodal dataset</strong>. 
      We started with <strong>AudioCaps</strong> (which provides audio and text), and then generated synthetic images from the captions using a text-to-image model.
      </p>
      <p>
      Our final system can learn from audio, images, and text together, and can even guess missing pieces from one modality using the others. This opens up many possibilities, like generating sounds from text or understanding audio with the help of images and captions.
      </p>
    </div>

    <div class="section" id="audio-tokenizer">
      <h2>Audio Tokenizer</h2>
  <p>
    As part of our effort to integrate audio into <strong>nano4M</strong>, we developed a tokenizer based on a
    <strong>Vector Quantized Variational Autoencoder (VQ-VAE)</strong>. The goal was to obtain discrete
    audio representations that can be used for multimodal learning alongside text and images.
  </p>

  <!-- === Architecture Diagrams === -->
  <div class="architecture-row">
    <h3>Model Architecture Overview</h3>
    <p>
      We experimented with three different audio tokenization architectures:
      (1) VQ-VAE using mel spectrogram input and Griffin-Lim decoding,
      (2) VQ-VAE trained directly on raw waveform,
      and (3) VQ-VAE combined with a WaveNet decoder.
    </p>
    <div class="architecture-row">
      <div class="audio-column">
        <p><strong>MelSpectrogram/Raw Waveform + L1/STFT Loss</strong></p>
        <img src="assets/images/vqvae/vqvae.jpg" alt="VQ-VAE using raw waveform with L1 or STFT loss" style="max-width: 100%;">
      </div>
      <div class="audio-column">
        <p><strong>Raw Waveform + WaveNet</strong></p>
        <img src="assets/images/vqvae/vqvae-wavenet.jpg" alt="VQ-VAE using WaveNet decoder" style="max-width: 100%;">
      </div>
    </div>
  </div>

  <!-- ==== [ 1. Spectrograms ] === --> 
  <h3>1. From Spectrograms to Tokens</h3>
  <p>
    We began by training a VQ-VAE on <a href="https://www.openslr.org/12" target="_blank">LibriSpeech (100h)</a>,
    using <em>mel spectrograms</em> as input features. While reconstruction losses appeared low, the
    resulting waveforms reconstructed using Griffin-Lim were highly distorted.
  </p>

  <!-- ==== [ Audio placeholder : Griffin] === --> 
  <h4>Audio Sample: Original vs Griffin-Lim Reconstruction</h4>
  <div class="audio-row">
    <div class="audio-column">
      <p><strong>Original</strong></p>
      <audio controls src="assets/audio/exp_vq/original_0.wav"></audio>
    </div>
    <div class="audio-column">
      <p><strong>Reconstructed (Griffin-Lim)</strong></p>
      <audio controls src="assets/audio/exp_vq/reconstruction_0_ststf.wav"></audio>
    </div>
  </div>

  </p>
    This indicated that small differences in mel spectrograms result in large perceptual errors,
    leading us to directly model waveforms instead.
  </p>

  <!-- ==== [ 1. Waveform ] === --> 
  <h3>2. Raw Waveform Modeling</h3>
  <p>
    We applied L1 loss between original and reconstructed waveforms. Despite improvements, the L1 loss
    failed to align with perceived audio quality. Adding a <strong>Short-Time Fourier Transform (STFT)</strong>
    loss helped, but did not fully resolve the issue.
  </p>

  <!-- ==== [ Audio placeholder : Raw wf] === --> 
  <h4>Audio Sample: Original vs VQ-VAE on raw waveform</h4>
  <div class="audio-row">
    <div class="audio-column">
      <p><strong>Original</strong></p>
      <audio controls src="assets/audio/original.wav"></audio>
    </div>
    <div class="audio-column">
      <p><strong>Reconstructed (VQ-VAE raw waveform)</strong></p>
      <audio controls src="assets/audio/vqvae.wav"></audio>
    </div>
  </div>

  <!-- ==== [ Image placeholder] === 
<h4>Training Loss </h4>
  <img src="assets/images/vqvae/trainingloss.png" alt="VQ-VAE training loss" style="max-width:100%; margin-top: 1em;">
   -->
  
  <!-- ==== [ 3. Wavenet ] === --> 
  <h3>3. WaveNet as Decoder</h3>
  <p>
    To achieve higher-quality reconstructions, we replaced the VQ-VAE decoder with a
    <strong>WaveNet</strong> conditioned on the quantized latents \(z_q(x)\). This autoregressive
    decoder models the waveform as a product of conditional distributions:
  </p>
  $$
  p(x|h) = \prod_{t=1}^{T} p(x_t | x_{1:t-1}, h),\text{ where } h = z_q(x)
  $$
  <p>
    WaveNet predicts parameters of a <em>mixture of logistic distributions</em> per timestep:
  </p>
  $$
  \begin{align}
    &p(x) =\sum_{i=1}^{K} \pi_i \cdot \text{Logistic}(x|\mu_i, \sigma_i) \\
    &Loss_{t} = -\log(p(x_{t}))\\
    &Loss_{total} = \sum_{t=1}^{T}Loss_{t}
  \end{align}
  $$


  <!-- ==== [ 4. Wavenet training ] === --> 
  <h3>4. Stabilizing Training</h3>
  <p>
    Initial training on the 100h subset was unstable. We added:
  </p>
  <ul>
    <li>Cosine Annealing with warm restarts</li>
    <li>Gradient Clipping</li>
    <li>360h of audio</li>
  </ul>
  <p>
    These changes partially stabilized learning. However, audio quality remained limited, likely due
    to insufficient latent expressiveness.
  </p>

  <!-- ==== [ Audio placeholder : Wavenet final] === --> 
 <h4>Audio Sample: Original vs reconstruction with Wavenet</h4>
  <div class="audio-row">
    <div class="audio-column">
      <p><strong>Original</strong></p>
      <audio controls src="assets/audio/original.wav"></audio>
    </div>
    <div class="audio-column">
      <p><strong>Reconstructed (VQ-VAE raw waveform)</strong></p>
      <audio controls src="assets/audio/wavenet.wav"></audio>
    </div>
  </div>

  <!-- ==== [ Image placeholder] === 
<h4>Training Loss of Wavenet</h4>
  <img src="assets/images/wavenetloss.png" alt="WaveNet training loss curve" style="max-width:100%; margin-top: 1em;">
   -->



  <!-- ==== [ Summary and future work ] === --> 
  <h3>7. Summary</h3>
  <p>
    Our experiments showed that high-quality waveform reconstruction from discrete tokens is
    challenging. L1 and STFT losses are insufficient alone; autoregressive models like WaveNet help,
    but depend heavily on latent quality and training stability.
  </p>
  <p>
    Future work includes exploring <strong>HiFi-GAN</strong> or <strong>WaveRNN</strong> as decoders, and adding perceptual
    losses for better alignment with human judgments.
  </p>
    </div>

    <div class="section" id="dataset">
      <h2>Dataset</h2>
      <p>
        A key challenge in our work was to find a suitable multimodal dataset containing aligned audio,
        images and text captions. Surprisingly, we did not find any publicly available dataset containing
        all three modalities. We ended up settling on <strong>AudioCaps</strong> dataset, containing,
        audio samples sourced from YouTube videos along with Human-writted captions describing the audio.
      </p>

      <p>
        While AudioCaps provides audio-caption pairs, it lacks corresponding aligned images.
        We considered extracting frames from the source YouTube videos, but this would violate YouTube's 
        Terms of Service and risk account termination.
        Instead, we generated synthetic images using a <strong>Dsitilled Stable Diffusion inference</strong>
        conditioned on the text captions.
      </p>

      <h3>Aligned audio, image and caption examples</h3>
      <div class="dataset-examples">
        <div class="example-card">
          <div>
            <img src="assets/images/dataset/101021.png" style="width: 100%;">
          </div>
          <div>
            <p><strong>Caption:</strong> "Rain falling and thunder roaring"</p>
            <audio controls src="assets/audio/dataset/101021.wav" style="width: 100%;"></audio>
          </div>
        </div>
      
        <div class="example-card">
          <div>
            <img src="assets/images/dataset/101096.png" style="width: 100%;">
          </div>
          <div>
            <p><strong>Caption:</strong> "Food frying with person narrating"</p>
            <audio controls src="assets/audio/dataset/101096.wav" style="width: 100%;"></audio>
          </div>
        </div>
      
        <div class="example-card">
          <div>
            <img src="assets/images/dataset/101162.png" style="width: 100%;">
          </div>
          <div>
            <p><strong>Caption:</strong> "Multiple adults speaking, and a child shouting in the background"</p>
            <audio controls src="assets/audio/dataset/101162.wav" style="width: 100%;"></audio>
          </div>
        </div>
      </div>

      
      <h3>Dataset Creation Pipeline</h3>
      <p>
        <ul>
          <li>Downloading the audio from and caption from
              <a href="https://huggingface.co/datasets/confit/audiocaps">AudioCaps</a></li>
          <li>Generating images using 
            <a href="https://huggingface.co/docs/diffusers/main/en/using-diffusers/distilled_sd">Distilled Stable Diffusion</a></li>
          <li>Tokenizing the images using
            <a href="https://huggingface.co/nvidia/Cosmos-0.1-Tokenizer-DI16x16">Cosmos-Tokenizer</a></li>
          <li>Tokenzing the audio using 
            <a href="https://github.com/facebookresearch/encodec">EnCodec</a></li>
        </ul>
      </p>
      
      <h3>Limitations</h3>
      <p>
        While our approach provided a solution to the lack of available dataset, it has some limitations.
        <strong>Synthetic images</strong> does not perfectly match the audio content and are sometimes
        unrecognizable. Furthermore, AudioCaps dataset contains mostly environmental sounds which due to 
        their uniqueness may be hard to learn and generate for a simple model.
      </p>

    <div class="section2">
      <h2>Nano4M & Extras</h2>
      <p>
      Once the dataset was fully tokenized, we trained our model using the <strong>nano4M architecture</strong>, which is based on a <strong>4M</strong>. During training, we intentionally hide a subset of the input tokens across modalities and ask the model to predict them using the available context.
      </p>
      <p>
        The diagram below illustrates this process. On the left, each input modalityâ€”image, audio, and textâ€”is tokenized separately. Then, a fixed number of tokens are randomly selected as inputs to the Transformer encoder, while the remaining ones are treated as targets to be predicted by the decoder. This masked pre-training encourages the model to reason across modalities.
      </p>
      <p>
        <img src="assets/images/vqvae/nano4m-pretraining.PNG" alt="Nano4M multimodal architecture" style="max-width:100%; margin-top: 1em; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
      </p>

      <p>
      This training strategyâ€”visualized aboveâ€”forces the model to reason across modalities to recover missing pieces. Whether itâ€™s a missing sound, word, or image patch, the model must infer it using cues from the other modalities.
      </p>
      <p>
        By repeating this process many times, the model learns to <strong>connect and understand</strong> how text, images, and audio relate to each other. This helps it build a <strong>shared representation</strong> of all three, and allows it to do things like <em>generate sound from text</em>, or <em>use audio to help interpret an image</em>.
      </p>
      <p>
        As a result, we obtained a <strong>lightweight but versatile model</strong> that can perform basic reasoning across modalities. Our early experiments did not yield significant results. The synthetic images generated from captions lacked sufficient visual clarity, which limited the model's ability to learn meaningful cross-modal representations. Additionally, the audio modality introduced a critical challenge: token granularity. While text and image data can be effectively represented within 256 tokens, this token budget captures only a small fraction of the audio signal. For instance, 5 seconds of audio at 48kHz using EnCodec typically results in approximately 3,000 tokens. As a result, during training, only a small segment of the audio is used, which leads the model to learn to generate short bursts of noise rather than coherent audio sequences..
      </p>
      <p>
        <strong>Below are some of the results we obtained during our evaluation:</strong>
      </p>
      <!-- RÃ©sultats Ã  ajouter ici -->
       <h3>RGB â†’ Caption</h3>
      <div class="container">
        <div class="image-box">
          <img src="assets/images/nano4m/sample1.png" alt="Input Image">
        </div>
        <div class="arrow">â†’</div>
        <div class="text-box">
          <p>"Papericking, then"</p>
        </div>
      </div>

      <!-- Flipped: Text left, Image right -->
       <h3>Caption â†’ RGB</h3>
    <div class="container reverse">
      <div class="image-box">
        <img src="assets/images/nano4m/sample2.png" alt="Output Image">
      </div>
      <div class="arrow">â†’</div>
      <div class="text-box">
        <p>"Someone crumples paper."</p>
      </div>
    </div>

      <!-- Audio to Text -->
       <h3>Audio â†’ RGB</h3>
  <div class="container">
    <div class="audio-box">
      <audio controls>
        <source src="assets/images/nano4m/audiorgb.wav" type="audio/wav">
        Your browser does not support the audio element.
      </audio>
    </div>
    <div class="arrow">â†’</div>
    <div class="image-box">
        <img src="assets/images/nano4m/audiorgb.png" alt="Output Image">
      </div>
  </div>

    </div>
    <div class="section3">
      <h2>Conclusion & Limitations</h2>
      <p>
  In this work, we explored the feasibility of integrating an audio modality into <strong>nano4M</strong>, 
  a lightweight multimodal architecture, using a fully synthetic dataset constructed from AudioCaps. 
  Our objective was to enable joint training across text, image, and audio modalities within a unified framework.
</p>

<p>
  Throughout our experiments, we encountered several key challenges:
</p>

<ul>
  <li>
    <strong>Synthetic Image Quality:</strong> The images generated from text captions lacked sufficient visual fidelity, 
    limiting the model's ability to learn robust cross-modal representations. While higher-quality generative tools could 
    mitigate this issue, our exploration was constrained by limited computational resources and time.
  </li>
  <li>
    <strong>Audio Tokenization:</strong> We implemented a VQ-VAE-based tokenizer and observed promising reconstruction results. 
    However, performance remains bounded by the scale and diversity of the training dataset. Expanding the dataset and including 
    more varied acoustic conditions could significantly improve audio representation quality.
  </li>
  <li>
    <strong>Token Granularity Mismatch:</strong> A core limitation lies in the significant disparity between token sequence lengths 
    across modalities. While text and image can typically be represented with fewer than 256 tokens, even short audio clips 
    (e.g., 5 seconds at 48kHz using EnCodec) result in thousands of tokens. This restricts the effective use of audio data 
    during training and hampers the modelâ€™s ability to learn meaningful cross-modal mappings. Addressing this granularity mismatch 
    remains a key direction for future work.
  </li>
</ul>

<p>
  Overall, our results highlight both the potential and the challenges of extending lightweight multimodal architectures 
  to include audio. This initial study lays the groundwork for more scalable and expressive models that can fully exploit 
  audio as a first-class modality.
</p>
    </div>

    <div class
