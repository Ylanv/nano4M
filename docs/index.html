<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>nano4M – Audio Modality Extension</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      line-height: 1.6;
      background-color: #f9f9f9;
      color: #333;
    }
    h1, h2, h3 {
      color: #2c3e50;
    }
    a {
      color: #2980b9;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    .section {
      margin-bottom: 40px;
    }
  </style>
</head>
<body>
  <h1>nano4M – Adding Audio Modality to a Multimodal Foundation Model</h1>

  <div class="section">
    <h2>Overview</h2>
    <p>This project extends <strong>nano4M</strong>, a compact variant of the <a href="#">4M</a> multimodal foundation model, by incorporating <strong>audio</strong> as an additional modality. Our goal is to push the boundaries of small-scale multimodal models by exploring whether audio integration during training enhances <strong>cross-modal transfer capabilities</strong> particularly in tasks such as <strong>Text-to-Speech (TTS)</strong>.</p>.</p>
  </div>

  <div class="section">
    <h2>Research Hypothesis</h2>
    <p>We hypothesize that training nano4M with audio inputs, in addition to vision and language, will:</p>
    <ul>
      <li>Improve the model’s <strong>generalization across modalities</strong>.</li>
      <li>Enable <strong>emergent capabilities</strong> in audio-based generation tasks.</li>
      <li>Showcase <strong>transfer learning potential</strong>, particularly in TTS applications.</li>
    </ul>
  </div>

  <div class="section">
    <h2>Methodology</h2>
    <h3>1. Audio Tokenization</h3>
    <p>We implemented a <a href="#">vector-quantized variational autoencoder (VQ-VAE)</a> to discretize raw audio into compact token sequences. This tokenizer enables audio to be treated similarly to text and image tokens during training.</p>

    <h3>2. Dataset Creation</h3>
    <p>We constructed a custom aligned multimodal dataset that includes:</p>
    <ul>
      <li>Text</li>
      <li>Corresponding audio</li>
      <li>Images generated to match the text descriptions</li>
    </ul>
    <p>This alignment enables the model to learn rich semantic correspondences across modalities.</p>

    <h3>3. Training</h3>
    <p>The nano4M model was trained on the aligned text-audio-image dataset using a unified autoregressive objective. The model was exposed to varied combinations of modalities to encourage <strong>multimodal reasoning</strong> and <strong>generation</strong>.</p>

    <h3>4. Evaluation</h3>
    <p>We evaluated the trained model on <strong>zero-shot and fine-tuned</strong> transfer to audio-based tasks. Particular focus was given to <strong>Text-to-Speech (TTS)</strong>, where the model’s ability to generate audio from text was assessed.</p>
  </div>

  <div class="section">
    <h2>Contributions</h2>
    <ul>
      <li>First integration of audio into the nano4M architecture.</li>
      <li>Custom VQ-based audio tokenizer adapted for multimodal training.</li>
      <li>Construction of a synthetic but semantically aligned multimodal dataset.</li>
      <li>Demonstration of audio transfer learning in a compact model setting.</li>
    </ul>
  </div>
</body>
</html>
