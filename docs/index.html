<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>nano4M â€“ Audio Modality Extension</title>
  <link rel="stylesheet" href="style.css">
  <script type="text/javascript"
    id="MathJax-script"
    async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

</head>
<body>

  <!-- Contenu principal centrÃ© -->
  <div class="wrapper">
    <div class="title-section">
      <h1>Integrating Audio into nano4M: 
        A Multimodal Extension</h1>
      <div class="authors">
        <a href="https://github.com/petritarifi" target="_blank">Petrit Arifi</a>,
        <a href="https://github.com/ozairfaizan" target="_blank">Ozair Faizan</a>,
        <a href="https://github.com/ylanv" target="_blank">Ylan Vifian</a>
      </div>
      <div class="affiliation">
        Swiss Federal Institute of Technology Lausanne (EPFL)

      </div>
      <div class="button-row">
        <a href="https://github.com/ylanv/nano4M" target="_blank" class="button">ðŸ”— Code</a>
      </div>
    </div>
  </div>

  <!-- Bandeau abstract -->
  <section class="abstract-section">
    <h2>Abstract</h2>
    <p>
      We extend <strong>nano4M</strong> by injecting an <em>audio</em> streamâ€”tokenised with our custom VQ-VAEâ€”to train a truly
      <em>multimodal</em> foundation model. This lightweight prototype shows that even small Transformer backbones can
      learn meaningful cross-modal representations, enabling zero-shot <abbr title="Text-to-Speech">TTS</abbr> and audio-conditioned image
      generation. We describe the audio tokenizer, the aligned dataset we built from AudioCaps, and the training
      tweaks that made the model converge despite limited compute.
    </p>
  </section>

  <!-- Bandeau overview -->
  <section class="overview-section">
    <h2>Overview</h2>
    <div class="overview-cards">
      <a class="card" href="#intro">Introduction</a>
      <a class="card" href="#audio-tokenizer">Audio tokenizer</a>
      <a class="card" href="#dataset">Dataset</a>
      <a class="card" href="#nanofm">Nano4M & Extras</a>
      <a class="card" href="#conclusion">Conclusion & Limitations</a>
    </div>
  </section>

  <!-- Contenu de sections (Ã  remplir si tu veux que les boutons scrollent vers quelque chose) -->
  <div class="wrapper">
    <div class="section" id="intro">
      <h2>Introduction</h2>
      <p>...</p>
    </div>

    <div class="section" id="audio-tokenizer">
      <h2>Audio Tokenizer</h2>
  <p>
    As part of our effort to integrate audio into <strong>nano4M</strong>, we developed a tokenizer based on a
    <strong>Vector Quantized Variational Autoencoder (VQ-VAE)</strong>. The goal was to obtain discrete
    audio representations that can be used for multimodal learning alongside text and images.
  </p>

  <!-- === Architecture Diagrams === -->
  <div class="architecture-row">
    <h3>Model Architecture Overview</h3>
    <p>
      We experimented with three different audio tokenization architectures:
      (1) VQ-VAE using mel spectrogram input and Griffin-Lim decoding,
      (2) VQ-VAE trained directly on raw waveform,
      and (3) VQ-VAE combined with a WaveNet decoder.
    </p>
    <div class="architecture-row">
      <div class="audio-column">
        <p><strong>Mel Spectrogram + Griffin-Lim</strong></p>
        <img src="assets/images/snoupy.jpg" alt="VQ-VAE using mel spectrogram and Griffin-Lim" style="max-width: 100%;">
      </div>
      <div class="audio-column">
        <p><strong>Raw Waveform + L1/STFT Loss</strong></p>
        <img src="assets/images/snoupy.jpg" alt="VQ-VAE using raw waveform with L1 or STFT loss" style="max-width: 100%;">
      </div>
      <div class="audio-column">
        <p><strong>Raw Waveform + WaveNet</strong></p>
        <img src="assets/images/snoupy.jpg" alt="VQ-VAE using WaveNet decoder" style="max-width: 100%;">
      </div>
    </div>
  </div>

  <!-- ==== [ 1. Spectrograms ] === --> 
  <h3>1. From Spectrograms to Tokens</h3>
  <p>
    We began by training a VQ-VAE on <a href="https://www.openslr.org/12" target="_blank">LibriSpeech (100h)</a>,
    using <em>mel spectrograms</em> as input features. While reconstruction losses appeared low, the
    resulting waveforms reconstructed using Griffin-Lim were highly distorted.
  </p>

  <!-- ==== [ Audio placeholder : Griffin] === --> 
  <h4>Audio Sample: Original vs Griffin-Lim Reconstruction</h4>
  <div class="audio-row">
    <div class="audio-column">
      <p><strong>Original</strong></p>
      <audio controls src="audio/original_mel.wav"></audio>
    </div>
    <div class="audio-column">
      <p><strong>Reconstructed (Griffin-Lim)</strong></p>
      <audio controls src="audio/reconstructed_griffin_lim.wav"></audio>
    </div>
  </div>

  </p>
    This indicated that small differences in mel spectrograms result in large perceptual errors,
    leading us to directly model waveforms instead.
  </p>

  <!-- ==== [ 1. Waveform ] === --> 
  <h3>2. Raw Waveform Modeling</h3>
  <p>
    We applied L1 loss between original and reconstructed waveforms. Despite improvements, the L1 loss
    failed to align with perceived audio quality. Adding a <strong>Short-Time Fourier Transform (STFT)</strong>
    loss helped, but did not fully resolve the issue.
  </p>

  <!-- ==== [ Audio placeholder : Raw wf] === --> 
  <h4>Audio Sample: Original vs VQ-VAE on raw waveform</h4>
  <div class="audio-row">
    <div class="audio-column">
      <p><strong>Original</strong></p>
      <audio controls src="audio/original_mel.wav"></audio>
    </div>
    <div class="audio-column">
      <p><strong>Reconstructed (VQ-VAE raw waveform)</strong></p>
      <audio controls src="audio/reconstructed_griffin_lim.wav"></audio>
    </div>
  </div>

  <!-- ==== [ Image placeholder] === --> 
<h4>Training Loss (Before and After Cosine Annealing)</h4>
  <img src="assets/images/snoupy.jpg" alt="WaveNet loss curve before cosine annealing" style="max-width:100%; margin-top: 1em;">
  <img src="assets/images/snoupy.jpg" alt="WaveNet loss curve after cosine annealing" style="max-width:100%; margin-top: 1em;">
  
  <!-- ==== [ 3. Wavenet ] === --> 
  <h3>3. WaveNet as Decoder</h3>
  <p>
    To achieve higher-quality reconstructions, we replaced the VQ-VAE decoder with a
    <strong>WaveNet</strong> conditioned on the quantized latents \(z_q(x)\). This autoregressive
    decoder models the waveform as a product of conditional distributions:
  </p>
  $$
  p(x|h) = \prod_{t=1}^{T} p(x_t | x_{1:t-1}, h),\text{ where } h = z_q(x)
  $$
  <p>
    The WaveNet predicts parameters of a <em>mixture of logistic distributions</em> per timestep:
  </p>
  $$
  \begin{align}
    &p(x) =\sum_{i=1}^{K} \pi_i \cdot \text{Logistic}(x|\mu_i, \sigma_i) \\
    &Loss_{t} = -\log(p(x_{t}))\\
    &Loss_{total} = \sum_{t=1}^{T}Loss_{t}
  \end{align}
  $$
 
  <!-- === WaveNet Loss Explanation === -->
  
  <h3>WaveNet Loss Visualization</h3>
    <p>
      The video below shows how each waveform point is modeled using a mixture of logistic distributions, and how the training loss is computed.
    </p>
    <video controls width="100%" poster="images/poster_loss_video.png">
      <source src="videos/wavenet_loss_explained.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
  

  <!-- ==== [ 4. Wavenet training ] === --> 
  <h3>4. Stabilizing Training</h3>
  <p>
    Initial training on the 100h subset was unstable. We added:
  </p>
  <ul>
    <li>Cosine Annealing with warm restarts</li>
    <li>Gradient Clipping</li>
  </ul>
  <p>
    These changes partially stabilized learning. However, audio quality remained limitedâ€”likely due
    to insufficient latent expressiveness.
  </p>

  <!-- ==== [ Audio placeholder : Wavenet final] === --> 
 <h4>Audio Sample: Original vs reconstruction with Wavenet</h4>
  <div class="audio-row">
    <div class="audio-column">
      <p><strong>Original</strong></p>
      <audio controls src="audio/original_mel.wav"></audio>
    </div>
    <div class="audio-column">
      <p><strong>Reconstructed (VQ-VAE raw waveform)</strong></p>
      <audio controls src="audio/reconstructed_griffin_lim.wav"></audio>
    </div>
  </div>

  <!-- ==== [ Image placeholder] === --> 
<h4>Training Loss of Wavenet</h4>
  <img src="assets/images/snoupy.jpg" alt="WaveNet loss curve before cosine annealing" style="max-width:100%; margin-top: 1em;">
  <img src="assets/images/snoupy.jpg" alt="WaveNet loss curve after cosine annealing" style="max-width:100%; margin-top: 1em;">

<!-- ==== [ Video full pipeline] === --> 
  <h3>Tokenizer training</h3>
    <p>
    This animation shows the entire audio transformation flow: from raw waveform input to discrete latent representation and back through WaveNet reconstruction.
    </p>
    <video controls width="100%" poster="images/poster_loss_video.png">
      <source src="videos/wavenet_loss_explained.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>

  <!-- ==== [ Summary and future work ] === --> 
  <h3>7. Summary</h3>
  <p>
    Our experiments showed that high-quality waveform reconstruction from discrete tokens is
    challenging. L1 and STFT losses are insufficient alone; autoregressive models like WaveNet help,
    but depend heavily on latent quality and training stability.
  </p>
  <p>
    Future work includes exploring <strong>HiFi-GAN</strong> or <strong>WaveRNN</strong> as decoders, and adding perceptual
    losses for better alignment with human judgments.
  </p>
    </div>

    <div class="section" id="dataset">
      <h2>Dataset</h2>
      <p>
        A key challenge in our work was to find a suitable multimodal dataset containing aligned audio,
        images and text captions. Surprisingly, we did not find any publicly available dataset containing
        all three modalities. We ended up settling on <strong>AudioCaps</strong> dataset, containing,
        audio samples sourced from YouTube videos along with Human-writted captions describing the audio.
      </p>

      <p>
        While AudioCaps provides audio-caption pairs, it lacks corresponding aligned images.
        We considered extracting frames from the source YouTube videos, but this would violate YouTube's 
        Terms of Service and risk account termination.
        Instead, we generated synthetic images using a <strong>Dsitilled Stable Diffusion inference</strong>
        conditioned on the text captions.
      </p>

      <h3>Aligned audio, image and caption examples</h3>
      <div class="dataset-examples">
        <div class="example-card">
          <div>
            <img src="assets/images/dataset/example1.jpg" style="width: 100%;">
          </div>
          <div>
            <p><strong>Caption:</strong> "..."</p>
            <audio controls src="assets/audio/dataset/example1.wav" style="width: 100%;"></audio>
          </div>
        </div>
      
        <div class="example-card">
          <div>
            <img src="assets/images/dataset/example2.jpg" style="width: 100%;">
          </div>
          <div>
            <p><strong>Caption:</strong> "..."</p>
            <audio controls src="assets/audio/dataset/example2.wav" style="width: 100%;"></audio>
          </div>
        </div>
      
        <div class="example-card">
          <div>
            <img src="assets/images/dataset/example3.jpg" style="width: 100%;">
          </div>
          <div>
            <p><strong>Caption:</strong> "..."</p>
            <audio controls src="assets/audio/dataset/example3.wav" style="width: 100%;"></audio>
          </div>
        </div>
      </div>

      
      <h3>Dataset Creation Pipeline</h3>
      <p>
        <ul>
          <li>Downloading the audio from and caption from
              <a href="https://huggingface.co/datasets/confit/audiocaps">AudioCaps</a></li>
          <li>Generating images using 
            <a href="https://huggingface.co/docs/diffusers/main/en/using-diffusers/distilled_sd">Distilled Stable Diffusion</a></li>
          <li>Tokenizing the images using
            <a href="https://huggingface.co/nvidia/Cosmos-0.1-Tokenizer-DI16x16">Cosmos-Tokenizer</a></li>
          <li>Tokenzing the audio using ...</li>
        </ul>
      </p>
      
      <h3>Limitations</h3>
      <p>
        While our approach provided a solution to the lack of available dataset, it has some limitations.
        <strong>Synthetic images</strong> does not perfectly match the audio content and are sometimes
        unrecognizable. Furthermore, AudioCaps dataset contains mostly environmental sounds which due to 
        their uniqueness may be hard to learn and generate for a simple model.
      </p>

    <div class="section" id="nanofm">
      <h2>Nano4M & Extras</h2>
      <p>
        We used the <strong>AudioCaps</strong> dataset mentioned above to train our multimodal model, <strong>nano4M</strong>. To do this, we applied three different tokenization strategies:
      </p>
      <ul>
        <li><strong>GPT-2</strong> for the text captions,</li>
        <li><strong>Cosmos</strong> for RGB images (patch-based tokenization),</li>
        <li>and our own <strong>VQ-VAE-based tokenizer</strong> for the audio.</li>
      </ul>
      
      <p>
      Once the dataset was fully tokenized, we trained our model using the <strong>nano4M architecture</strong>, which is based on a <strong>masked Transformer</strong>. During training, we intentionally hide a subset of the input tokens across modalities and ask the model to predict them using the available context.
      </p>
      <p>
        The diagram below illustrates this process. On the left, each input modalityâ€”image, audio, and textâ€”is tokenized separately. Then, a fixed number of tokens are randomly selected as inputs to the Transformer encoder, while the remaining ones are treated as targets to be predicted by the decoder. This masked pre-training encourages the model to reason across modalities.
      </p>
      <p>
        <img src="assets/images/vqvae/nano4m-pretraining.PNG" alt="Nano4M multimodal architecture" style="max-width:100%; margin-top: 1em; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
      </p>

      <p>
      This training strategyâ€”visualized aboveâ€”forces the model to reason across modalities to recover missing pieces. Whether itâ€™s a missing sound, word, or image patch, the model must infer it using cues from the other modalities.
      </p>
      <p>
        By repeating this process many times, the model learns to <strong>connect and understand</strong> how text, images, and audio relate to each other. This helps it build a <strong>shared representation</strong> of all three, and allows it to do things like <em>generate sound from text</em>, or <em>use audio to help interpret an image</em>.
      </p>
      <p>
        As a result, we obtained a <strong>lightweight but versatile model</strong> that can perform basic reasoning across modalities. Our early experiments show promising results, especially in tasks like <em>text-to-audio generation</em> and <em>cross-modal completion</em>.
      </p>
      <p>
        <strong>Below are some of the results we obtained during our evaluation:</strong>
      </p>
      <!-- RÃ©sultats Ã  ajouter ici -->

    </div>
    <div class="section" id="conclusion">
      <h2>Conclusion & Limitations</h2>
      <p>...</p>
    </div>

    <div class
